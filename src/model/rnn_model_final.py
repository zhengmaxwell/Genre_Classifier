# -*- coding: utf-8 -*-
"""RNN_Model_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A66V10F3GeRCbtGCwzQ0fedxPexJ6NB-

Import Libraries and Mount Drive
"""

import torch
import csv
import time
import torchtext
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import random
import matplotlib.pyplot as plt
from torch.nn.utils.rnn import pad_sequence
from google.colab import drive
from collections import defaultdict
drive.mount('/content/drive')

###### IMPORTANT #####

embeddingSize = 200

##### IMPORTANT #####

"""Loading Training, Validation and Test Data"""

##### Loading Data #####
#creates the dataset given the cleanData.csv file on drive. note: cleanData.csv CANNOT be in a shared folder, it must be in your personal drive

text_field = torchtext.data.Field(sequential=True,      # text sequence
                                  tokenize=lambda x: x, # because are building a character-RNN
                                  include_lengths=True, # to track the length of sequences, for batching
                                  batch_first=True,
                                  use_vocab=True)       # to turn each character into an integer index
label_field = torchtext.data.Field(sequential=False,    # not a sequence
                                   use_vocab=False,     # don't need to track vocabulary
                                   is_target=True,      
                                   batch_first=True) # convert text to 0 and 1

path_tian = "/content/gdrive/My Drive/Colab Notebooks/APS360 Project"
path_alisha = "/content/drive/My Drive/4th year/APS360"
path_samin = "/content/gdrive/My Drive/Colab Notebooks/APS360/Project"

fields = [('label', label_field), ('desc', text_field)]

trainDataSet = torchtext.data.TabularDataset(path_alisha+ "/trainCleanData.csv", # name of the file
                                        "csv",               # fields are separated by a comma
                                        fields)
valDataSet = torchtext.data.TabularDataset(path_alisha+ "/valCleanData.csv", # name of the file
                                        "csv",               # fields are separated by a comma
                                        fields)

## if you're loading in test data
testDataSet = torchtext.data.TabularDataset(path_alisha+ "/testCleanData.csv", # name of the file
                                       "csv",               # fields are separated by a comma
                                       fields)

"""Data Formatting"""

##### prelim data formatting, creation of vocab and iterator objects #####

# splits the description property of the dataset into a list of words. Needed to build word based vocab. Otherwise vocab is built with just characters
# ex: "this is a description" => ["this", "is", "a", "description"]
genreDict = {'horror' : 0 , 'mystery' : 1, 'romance' : 2, 'fantasy' : 3, 'science fiction' : 4}

#final preprocessing on the training data set
if (type(trainDataSet[0].desc) is str):
  for i in range (0, len(trainDataSet)):
    trainDataSet[i].label = genreDict[trainDataSet[i].label] #changes each label into a number - crossentropyloss doesn't take onehot, it takes index based
    trainDataSet[i].desc = trainDataSet[i].desc.split() #splitting words in descriptions into a list for build_vocab to work

#final preprocessing on the validation data set
if (type(valDataSet[0].desc) is str):
  for i in range (0, len(valDataSet)):
    valDataSet[i].label = genreDict[valDataSet[i].label] 
    valDataSet[i].desc = valDataSet[i].desc.split() 

#build a vocab (basically assign each word to an index)
text_field.build_vocab(trainDataSet)

# bucketiterators created, needed to create batches that have similar lengths (minimize padding)
train_iter = torchtext.data.BucketIterator(trainDataSet,
                                           batch_size=10,
                                           sort_key=lambda x: len(x.desc), # to minimize padding
                                           sort_within_batch=True,        # sort within each batch
                                           repeat=False)                  # repeat the iterator for many epochs

val_iter = torchtext.data.BucketIterator(valDataSet,
                                           batch_size=10,
                                           sort_key=lambda x: len(x.desc), 
                                           sort_within_batch=True,        
                                           repeat=False)

"""Loading Pre-trained GloVe Embeddings"""

def loadGloveModel(File):
    print("Loading Glove Model")
    f = open(File,'r')
    gloveModel = defaultdict(lambda: torch.zeros(embeddingSize, dtype=torch.float64))
    for line in f:
        splitLines = line.split()
        word = splitLines[0]
        wordEmbedding = torch.from_numpy(np.array([float(value) for value in splitLines[1:]]))
        gloveModel[word] = wordEmbedding
    print(len(gloveModel)," words loaded")
    return gloveModel

model_path = model_path = "/content/drive/My Drive/4th year/APS360/glove.twitter.27B/200d.txt" #use 50d.txt, 100d.txt, 200d.txt for alt. dimensions
glove = loadGloveModel(model_path)

"""Visualize Data (for debugging)"""

#debug code
print(text_field.vocab.stoi)
print (len(text_field.vocab.stoi))
#print(text_field.vocab.itos[5])

#debug code / visualization of data, print values in dataset
for count, i in enumerate(trainDataSet):
  print("label: {}, data: {}".format(i.label, i.desc))
  if count == 10: #print first 10 pieces of data
    break

#debug code to print the length of batches
counter = 0
for batch in train_iter:
    #print(len(batch))
    print(batch.desc)
    print ("maximum length in batch {}: {}". format (counter, batch.desc[1].numpy()[0]))
    padcounter = 0
    for i in batch.desc[0]:
      for j in i:
        if (j.numpy() == 1):
          padcounter += 1
    
    print ("total number of pads used in batch {}: {}".format( counter, padcounter))
    padcounter = 0
    counter += 1
    if (counter == 10):
      break

"""Helper Functions"""

##### Data formatting and debug helper functions #####

def toOneHot (sample, labels):
  output = np.zeros(len(labels))
  for count, i in enumerate(labels):
    if i == sample:
      output [count] = 1
      return output

def print_closest_words(vec, n=5): 
    #made minor modifications so it can return the word itself. we can use to determine what a word is based on its glove embedding
    dists = torch.norm(glove.vectors - vec, dim=1)     # compute distances to all words
    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # sort by distance
    for idx, difference in lst[0:n+1]:                         # take the top n including the word itelf
        print(glove.itos[idx], difference)

def gloveToWords(embedded):
  #given an array of glove embeddings, print the corresponding words
  for i in embedded:
    print_closest_words(i, 0)

def gloveToWords2 (lookup_value): #new glove vectors have embeddings for other words - interesting.
  words = ""
  for key, value in glove.items():
      if (value[0] == lookup_value[0]):
          words = key
          break
  print (words)

##### Helper functions used during training and validation #####

def toEmbedded (batch): 
  #converts a pytorch tensor batch embedded in our vocab embedding into a pytorch tensor batch embedded using glove

  count = 0
  dim1 = list(np.shape(batch)) [0]
  dim2 = list(np.shape(batch)) [1]
  embBatch = torch.empty(dim1, dim2, embeddingSize) #creating an empty tensor to recieve the new data
  for i, sample in enumerate (batch):
    for j, index in enumerate (sample):
      embBatch[i][j] = glove[text_field.vocab.itos[index]] #this line basically does the entire conversion
  return embBatch

def get_stats(model, data_loader, criterion, lossEnable = False): 
  #gets accuracy and loss (if lossenable is set to true manually)
  #returns it in a list with the form [loss, accuracy]

  correct, total, totalLoss, lossNums = 0, 0, 0, 0
  for desc, labels in data_loader:
    #converts data to glove embeddings and feeds into model
    embDesc = toEmbedded(desc[0])
    pred = model(embDesc)

    #determine accuracy
    value, index = pred.max(1) #get index of max value for each prediction (prediction gives a torch tensor array of probabilities)
    correct += ((labels == index).sum()).item()

    #determine loss if desired. Lossenable used to save computational time in case we don't need it
    if lossEnable:
      loss = float(criterion(pred, labels)) #casting to float now to reduce memory usage
      totalLoss += loss

    total += labels.shape[0]
    lossNums += 1
  return [totalLoss / lossNums, correct / total]

import torch
def getConfusionMatrix(model, data_loader):
  numSamples = len(data_loader)
  correct = 0

# #genres: fantasy, romance, horror, mystery, science fiction
# genreDict = {0 : "fantasy", 1 : "romance", 2 : "horror", 3 : "mystery", 4 : "science fiction"}
  genre = ['horror', 'mystery', 'romance', 'fantasy', 'science fiction']

  fantasyAcc = [-1,-1,-1,-1,-1]
  romanceAcc = [-1,-1,-1,-1,-1]
  horrorAcc = [-1,-1,-1,-1,-1]
  mysteryAcc = [-1,-1,-1,-1,-1]
  sciFiAcc = [-1,-1,-1,-1,-1]

  totalAcc = [horrorAcc, mysteryAcc,  romanceAcc, fantasyAcc, sciFiAcc]

  for desc, labels in data_loader:
#for i in range (0, numSamples):
  #index = random.randrange(0, numSamples, 1)
    embDesc = toEmbedded(desc[0])
    pred = model(embDesc)
    value, index = pred.max(1)
    index = index.item()
    labels = labels.item()
    prediction = genre[index]
    real = genre[labels]
   
    if (prediction == real):
      correct += 1

    if totalAcc[labels][index] == -1:
      totalAcc[labels][index] = 1
    else:
      totalAcc[labels][index] += 1
  
  #get fractions
  for genreAcc in totalAcc:
    total = sum(genreAcc)
    for i, val in enumerate(genreAcc):
      genreAcc[i] = val/total


  print("accuracy: {}".format(correct/numSamples))
  for i, genreAcc in enumerate(totalAcc):
    print("actual: {}".format(genre[i]), ", predicted as: {}".format(genreAcc))

"""RNN Model Architecture"""

# #####  model architecture #####
class GenreRNN(nn.Module):
    def __init__(self, input_size=200, hidden_size=150, num_classes=5):
        super(GenreRNN, self).__init__()
        self.name = "GenreRNN"
        self.hidden_size = hidden_size
        self.num_classes = num_classes
        #self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)
        #self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x):
        # Set an initial hidden state
        h0 = torch.zeros(1, x.size(0), self.hidden_size)
        #c0 = torch.zeros(1, x.size(0), self.hidden_size)
        # Forward propagate the RNN
        #out, _ = self.rnn(x, h0)
        out, _ = self.gru(x, h0)
        #out, (h0, c0) = self.lstm(x, (h0, c0))
        # Pass the output of the last time step to the classifier
        out = self.fc(out[:, -1, :])
        return out

"""Training"""

##### Training Code #####
def train_rnn_network(model, train, valid, num_epochs=5, learning_rate=1e-4):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.94)
    train_loss, valid_loss, train_acc, valid_acc = [], [], [], []
    epochs = []

    for epoch in range(num_epochs):
        embeddingTime, predictionTime, optimizerTime, bookKeepingTime = 0, 0, 0, 0
        cumuLoss, batchCounter, totalTweets, correct = 0, 0, 0, 0
        for desc, labels in train:
            time1 = time.time() 
            optimizer.zero_grad()
            embDesc = toEmbedded(desc[0]) #change data from our vocab to glove embeddings
            time2 = time.time() #tracking time from beginning to after embeddings done

            #standard training code
            pred = model(embDesc)
            loss = criterion(pred, labels)
            time3 = time.time() #tracking how long it takes to get prediction + loss

            loss.backward()
            optimizer.step()
            time4 = time.time() #tracking how long it takes to do backprop

            value, index = pred.max(1) #get index of max value for each prediction (prediction gives a torch tensor array of probabilities)
            correct += ((labels == index).sum()).item()
            cumuLoss += float(loss) #cumulatively adds loss for every tweet batch
            batchCounter += 1 #counts number of tweet batches
            totalTweets += labels.shape[0]
            time5 = time.time() #tracking all of the bookkeeping

            embeddingTime += (time2 - time1)
            predictionTime += (time3 - time2)
            optimizerTime += (time4 - time3)
            bookKeepingTime += (time5 - time4)

        time6 = time.time()
        train_loss.append(cumuLoss/ batchCounter) #averages cumulative loss over number of tweets
        train_acc.append(correct / totalTweets)

        valid_stats = get_stats(model, valid, criterion, True)
        valid_loss.append(valid_stats[0])
        valid_acc.append(valid_stats[1])

        epochs.append(epoch)
        time7 = time.time() #tracking the time needed to get validation results

        validTime = time7 - time6

        print("Epoch %d; Train Loss %f; Val Loss %f; Train Acc %f; Val Acc %f| embTime %f; predTime %f; optTime %f; bookTime %f; valTime %f" % (
              epoch+1, train_loss[-1], valid_loss[-1], train_acc[-1], valid_acc[-1], embeddingTime, predictionTime, optimizerTime, bookKeepingTime, validTime))
        
        model_name = "{}, learning rate {}, epoch{}".format(model.name, learning_rate, epoch + 1)

        #if (epoch > num_epochs * 1/5): #saving the outputs (in the final 1/5 of epochs so it doesn't use too much storage)

        SavePath = "/content/drive/My Drive/4th year/APS360/Saved Models/"

        torch.save(model.state_dict(), SavePath + "{}".format(model_name))
        np.savetxt(SavePath + "{}_train_acc.csv".format(model_name), train_acc)
        np.savetxt(SavePath + "{}_train_loss.csv".format(model_name), train_loss)
        np.savetxt(SavePath + "{}_valid_acc.csv".format(model_name), valid_acc)
        np.savetxt(SavePath + "{}_valid_loss.csv".format(model_name), valid_loss)


    #plotting
    plt.title("Training Curve")
    plt.plot(epochs, train_loss, label="Train")
    plt.plot(epochs, valid_loss, label="Validation")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend(loc='best')
    plt.show()

    plt.title("Training Curve")
    plt.plot(epochs, train_acc, label="Train")
    plt.plot(epochs, valid_acc, label="Validation")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend(loc='best')
    plt.show()

"""Hyperparameter Tuning"""

#TRY 1
final_model = GenreRNN()

train_rnn_network(final_model, train_iter, val_iter, num_epochs = 30, learning_rate=0.0001)

#TRY 2 - batch size 5, hidden 100
final_model = GenreRNN()

train_rnn_network(final_model, train_iter, val_iter, num_epochs = 45, learning_rate=0.001)

#TRY 3 - batch size 5, hidden 150, no dropout
final_model = GenreRNN()

train_rnn_network(final_model, train_iter, val_iter, num_epochs = 45, learning_rate=0.0001)

#TRY 4 - batch size 10, hidden 50, no dropout 
final_model = GenreRNN()
train_rnn_network(final_model, train_iter, val_iter, num_epochs = 25, learning_rate=0.0001)

#Try 5 - batch size 10, hidden 150, no dropout, lr 0.0001, epochs 14
final_modelFINAL = GenreRNN()
train_rnn_network(final_model, train_iter, val_iter, num_epochs = 14, learning_rate=0.0001)

#retrieving final trained model from saved folder
finalRNN = GenreRNN()
state = torch.load("/content/drive/My Drive/4th year/APS360/finalTrainedModel")
testRNN.load_state_dict(state)

"""Load Untouched Test Data"""

#Loading Test Data to determine test accuracy
#final preprocessing on the testing data set
if (type(testDataSet[0].desc) is str):
  for i in range (0, len(testDataSet)):
    testDataSet[i].label = genreDict[testDataSet[i].label] #changes each label into a number - crossentropyloss doesn't take onehot, it takes index based
    testDataSet[i].desc = testDataSet[i].desc.split() #splitting words in descriptions into a list for build_vocab to work


# bucketiterators created, needed to create batches that have similar lengths (minimize padding)
test_iter = torchtext.data.BucketIterator(testDataSet,
                                           batch_size=1,
                                           sort_key=lambda x: len(x.desc), # to minimize padding
                                           sort_within_batch=True,        # sort within each batch
                                           repeat=False)                  # repeat the iterator for many epochs

"""Get Final Test Accuracy"""

#get final test accuracy
criterion = nn.CrossEntropyLoss()
print("test accuracy of final model : ", get_stats(finalRNN, test_iter, criterion))

"""Confusion Matrix"""

#get Confusion Matrix for model using test data
getConfusionMatrix(testRNN, test_iter)